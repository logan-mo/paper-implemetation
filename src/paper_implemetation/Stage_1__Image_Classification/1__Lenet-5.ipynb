{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes from the paper:\n",
    "\n",
    "The Lenet paper summrizes previous work done on character recognition, including SGD, Convolutions and Neural Networks.\n",
    "\n",
    "Goal:\n",
    "Character Recognition, by building a character classifier\n",
    "\n",
    "Dataset Used:\n",
    "MNIST\n",
    "\n",
    "Method Used:\n",
    "Build a Convolution based Feature Extractor, followed by a Fully Connected Neural Network Classifier\n",
    "\n",
    "Architecture:\n",
    "Input (32, 32)\n",
    "-> Convolution (5x5, 6 filters) (6, 28, 28)\n",
    "-> Sub Sampling (6, 14, 14)\n",
    "-> Sigmoid\n",
    "-> Convolution (5x5, 16 filters) (16, 10, 10)\n",
    "-> Sub Sampling (16, 5, 5)\n",
    "-> Sigmoid\n",
    "-> Convolution (5x5, 120 filters) (120, 1, 1)\n",
    "-> Sigmoid\n",
    "-> Fully Connected (120)\n",
    "-> Sigmoid\n",
    "-> Fully Connected (84)\n",
    "-> Sigmoid\n",
    "-> RBF (10)\n",
    "\n",
    "Training Parameters / Hyperparamters:\n",
    "- Important to note detail is the the dataset is 28 x 28. Padding is added to the image to better extract stroke-endpoints on the edges on the images\n",
    "- Image is norrmalized to have zero mean and equal variance.\n",
    "- Sumsampling means, in a 2x2 pixel area, all values are arred, multiplied by a weight and added to a bias. This IS NOT THE SAME AS MAX POOLING.\n",
    "- Stride for subsampling is 2, so that the output is half the size of the input and the area of sub-sampling is non overlaping\n",
    "- S2 and C3 have some weird associations which I will ignore probably\n",
    "- The last layer is a layer of RBF units instead of neurons. The Paper explains, \"In probabilistic terms, the RBF output can be interpreted as the unnormalized negative loglikelihood of a Gaussian distribution in the space of configurations of layer F6\"\n",
    "- Loss function is MSE, but they modify it and make it scary. We will just just MSE loss\n",
    "\n",
    "- Ran three Experiments\n",
    "- 1. Images were centered into a 28 x 28 image and then padded to 32 x 32. This was called the \"Regular\" dataset\n",
    "- 2. Images were deslanted and cropped into a 20 x 20 image. This was called the \"Deslanted\" dataset\n",
    "- 3. Images were centered into a 16 x 16 image. The Author forgot to name this dataset like it was his middle child.\n",
    "\n",
    "I will only be using the Regular Dataset.\n",
    "\n",
    "- Trained for 20 epochs\n",
    "- 60k training images, 10k test images\n",
    "- Learning Rate was 0.0005 for the first 2 epochs, and 0.0002 for the next 3, 0.0001 fir the next 3, 0.00005 for the next 4 and 0.00001 thereafter.\n",
    "- Author obeserver no over-fitting? Is he Jesus? The Author says this is because the learning rates are too high? LMFAO\n",
    "- \n",
    "\n",
    "Metrics Defined:\n",
    "Error Rate\n",
    "- Number of misclassified test samples / Total number of test samples\n",
    "\n",
    "Results:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input (32, 32)\n",
    "# -> Convolution (5x5, 6 filters) (6, 28, 28)\n",
    "# -> Sub Sampling (6, 14, 14)\n",
    "# -> Sigmoid\n",
    "# -> Convolution (5x5, 16 filters) (16, 10, 10)\n",
    "# -> Sub Sampling (16, 5, 5)\n",
    "# -> Sigmoid\n",
    "# -> Convolution (5x5, 120 filters) (120, 1, 1)\n",
    "# -> Sigmoid\n",
    "# -> Fully Connected (120\n",
    "# )\n",
    "# -> Sigmoid\n",
    "# -> Fully Connected (84)\n",
    "# -> Sigmoid\n",
    "# -> RBF (10)\n",
    "\n",
    "# Since the Sub Sampling as mentioned by Yunn LeCun is not the same as Average Pooling, I will implement it as a trainable layer\n",
    "class TrainableAvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        super(TrainableAvgPool2d, self).__init__()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size, stride=stride)\n",
    "        # Learnable weight and bias\n",
    "        self.weight = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "# Well guess what, RBFs are also extinct. So I gotta implement my own\n",
    "class RBFLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, gamma=1.0):\n",
    "        super(RBFLayer, self).__init__()\n",
    "        # Learnable RBF centers with shape [output_dim, input_dim]\n",
    "        self.centers = nn.Parameter(torch.randn(output_dim, input_dim))\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute squared Euclidean distance between input and RBF centers\n",
    "        # x shape: [batch_size, input_dim], centers shape: [output_dim, input_dim]\n",
    "        dists = torch.cdist(x.unsqueeze(1), self.centers.unsqueeze(0)) ** 2\n",
    "        # Apply Gaussian function to get RBF output\n",
    "        rbf_out = torch.exp(-self.gamma * dists.squeeze(1))\n",
    "        return rbf_out\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = TrainableAvgPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = TrainableAvgPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.rbf = RBFLayer(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.s2(self.c1(x)))\n",
    "        x = self.s4(self.c3(x))\n",
    "        x = self.c5(x)\n",
    "        x = x.view(-1, 120)\n",
    "        x = self.f6(x)\n",
    "        x = self.rbf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test the model class\n",
    "\n",
    "model = Lenet5()\n",
    "image = torch.randn(5, 1, 28, 28)\n",
    "\n",
    "output = model(image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, one_hot(torch.tensor(label), num_classes=10).float()\n",
    "\n",
    "\n",
    "# normalize the image so that black corresponds to 0.1 and white corresponds to 1.175\n",
    "# These numbers are taken directly from the paper and Author claims these give us a mean of 0 and std of 1\n",
    "def custom_normalize(img: torch.Tensor, target_min = -0.1, target_max = 1.175) -> torch.Tensor:\n",
    "    # Assuming img is a torch tensor with pixel values in [0, 255]\n",
    "    img = img.float()  # Ensure the tensor is float for proper scaling\n",
    "    img = (img - 0) * (target_max - target_min) / (255 - 0) + target_min\n",
    "    return img\n",
    "    \n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(custom_normalize)\n",
    "])\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x, y = mnist_train_dataset[0]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Custom LR schedule as per the paper\n",
    "# We set an initial Learning RAte and the scheduler will adjust it based on the epoch\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0  # 0.0005 (initial LR)\n",
    "    elif epoch < 5:\n",
    "        return 0.4  # 0.0002\n",
    "    elif epoch < 8:\n",
    "        return 0.2  # 0.0001\n",
    "    elif epoch < 12:\n",
    "        return 0.1  # 0.00005\n",
    "    else:\n",
    "        return 0.02  # 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 20\n",
    "optimizer = SGD(model.parameters(), lr=0.0005)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    # Convert y_true from one-hot to class indices\n",
    "    y_true_classes = y_true.argmax(dim=1)\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true_classes).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "Epoch 1: Test Loss = 0.100000, Train Loss = 0.100000 <br>\n",
    "Epoch 1: Test Accuracy = 0.0978, Train Accuracy = 0.0987 <br>\n",
    "Epoch 2: Test Loss = 0.100000, Train Loss = 0.100000 <br>\n",
    "Epoch 2: Test Accuracy = 0.0978, Train Accuracy = 0.0987 <br>\n",
    "Epoch 3: Test Loss = 0.100000, Train Loss = 0.100000 <br>\n",
    "Epoch 3: Test Accuracy = 0.0978, Train Accuracy = 0.0987 <br>\n",
    "Epoch 4: Test Loss = 0.100000, Train Loss = 0.100000 <br>\n",
    "Epoch 4: Test Accuracy = 0.0978, Train Accuracy = 0.0987 <br>\n",
    "Epoch 5: Test Loss = 0.100000, Train Loss = 0.100000 <br>\n",
    "Epoch 5: Test Accuracy = 0.0978, Train Accuracy = 0.0987 <br>\n",
    "Epoch 6: Test Loss = 0.100000, Train Loss = 0.100000 <br>\n",
    "Epoch 6: Test Accuracy = 0.0978, Train Accuracy = 0.0987 <br>\n",
    "Epoch 7: Test Loss = 0.100000, Train Loss = 0.100000 <br>\n",
    "Epoch 7: Test Accuracy = 0.0978, Train Accuracy = 0.0987 <br>\n",
    "Epoch 8: Test Loss = 0.100000, Train Loss = 0.100000 <br>\n",
    "Epoch 8: Test Accuracy = 0.0978, Train Accuracy = 0.0987 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Paper replication clearly failed. The main reason being that pytorch modules assume modern standards are bsing used. And also because of vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvemnt 1: Increase the learning rate and remove the decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 1: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 2: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 2: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 3: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 3: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 4: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 4: Test Accuracy = 0.1136, Train Accuracy = 0.1123\n",
      "Epoch 5: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 5: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "class TrainableAvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        super(TrainableAvgPool2d, self).__init__()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size, stride=stride)\n",
    "        self.weight = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "class RBFLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, gamma=1.0):\n",
    "        super(RBFLayer, self).__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(output_dim, input_dim))\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x):\n",
    "        dists = torch.cdist(x.unsqueeze(1), self.centers.unsqueeze(0)) ** 2\n",
    "        rbf_out = torch.exp(-self.gamma * dists.squeeze(1))\n",
    "        return rbf_out\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = TrainableAvgPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = TrainableAvgPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.rbf = RBFLayer(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.s2(self.c1(x)))\n",
    "        x = self.s4(self.c3(x))\n",
    "        x = self.c5(x)\n",
    "        x = x.view(-1, 120)\n",
    "        x = self.f6(x)\n",
    "        x = self.rbf(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, one_hot(torch.tensor(label), num_classes=10).float()\n",
    "\n",
    "\n",
    "def custom_normalize(img: torch.Tensor, target_min = -0.1, target_max = 1.175) -> torch.Tensor:\n",
    "    img = img.float()  # Ensure the tensor is float for proper scaling\n",
    "    img = (img - 0) * (target_max - target_min) / (255 - 0) + target_min\n",
    "    return img\n",
    "    \n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(custom_normalize)\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Convert y_true from one-hot to class indices\n",
    "    y_true_classes = y_true.argmax(dim=1)\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true_classes).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement #2 : Update the model architecture <br>\n",
    "- Replace Sigmod with ReLU activation function\n",
    "- Replace the RBF layer with a fully connected layer\n",
    "- Add activation on the last layer\n",
    "- Replace the trainable Average pool with a maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 0.205365, Train Loss = 0.231430\n",
      "Epoch 1: Test Accuracy = 0.1011, Train Accuracy = 0.1022\n",
      "Epoch 2: Test Loss = 0.090314, Train Loss = 0.141966\n",
      "Epoch 2: Test Accuracy = 0.0983, Train Accuracy = 0.0993\n",
      "Epoch 3: Test Loss = 0.089986, Train Loss = 0.090044\n",
      "Epoch 3: Test Accuracy = 0.1136, Train Accuracy = 0.1015\n",
      "Epoch 4: Test Loss = 0.089972, Train Loss = 0.089980\n",
      "Epoch 4: Test Accuracy = 0.1136, Train Accuracy = 0.1123\n",
      "Epoch 5: Test Loss = 0.089969, Train Loss = 0.089974\n",
      "Epoch 5: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = nn.MaxPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.f7 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.s2(self.c1(x)))\n",
    "        x = F.relu(self.s4(self.c3(x)))\n",
    "        x = F.relu(self.c5(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.relu(self.f6(x))\n",
    "        x = self.f7(x)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, one_hot(torch.tensor(label), num_classes=10).float()\n",
    "\n",
    "\n",
    "def custom_normalize(img: torch.Tensor, target_min = -0.1, target_max = 1.175) -> torch.Tensor:\n",
    "    img = img.float()  # Ensure the tensor is float for proper scaling\n",
    "    img = (img - 0) * (target_max - target_min) / (255 - 0) + target_min\n",
    "    return img\n",
    "    \n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(custom_normalize)\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Convert y_true from one-hot to class indices\n",
    "    y_true_classes = y_true.argmax(dim=1)\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true_classes).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement #3: Change the Loss to CrossEntropyLoss <br>\n",
    "To do that, Remove the activation layer on the last layer of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 2.301208, Train Loss = 2.302107\n",
      "Epoch 1: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 2: Test Loss = 2.301031, Train Loss = 2.301321\n",
      "Epoch 2: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 3: Test Loss = 2.301044, Train Loss = 2.301251\n",
      "Epoch 3: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 4: Test Loss = 2.301025, Train Loss = 2.301240\n",
      "Epoch 4: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 5: Test Loss = 2.301021, Train Loss = 2.301231\n",
      "Epoch 5: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = nn.MaxPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.f7 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.s2(self.c1(x)))\n",
    "        x = F.relu(self.s4(self.c3(x)))\n",
    "        \n",
    "        x = F.relu(self.c5(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.relu(self.f6(x))\n",
    "        x = self.f7(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def custom_normalize(img: torch.Tensor, target_min = -0.1, target_max = 1.175) -> torch.Tensor:\n",
    "    img = img.float()  # Ensure the tensor is float for proper scaling\n",
    "    img = (img - 0) * (target_max - target_min) / (255 - 0) + target_min\n",
    "    return img\n",
    "    \n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(custom_normalize)\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement #4: Change the normalization to 0-1 instead of the whacky number.<br>\n",
    "Not normalizing to mean 0 and std 1, because all rely layers zero all negative outputs anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 2.179829, Train Loss = 2.282292\n",
      "Epoch 1: Test Accuracy = 0.4022, Train Accuracy = 0.1871\n",
      "Epoch 2: Test Loss = 0.322339, Train Loss = 0.796592\n",
      "Epoch 2: Test Accuracy = 0.9026, Train Accuracy = 0.7783\n",
      "Epoch 3: Test Loss = 0.185626, Train Loss = 0.273590\n",
      "Epoch 3: Test Accuracy = 0.9455, Train Accuracy = 0.9151\n",
      "Epoch 4: Test Loss = 0.139431, Train Loss = 0.184453\n",
      "Epoch 4: Test Accuracy = 0.9570, Train Accuracy = 0.9439\n",
      "Epoch 5: Test Loss = 0.115000, Train Loss = 0.142690\n",
      "Epoch 5: Test Accuracy = 0.9645, Train Accuracy = 0.9564\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = nn.MaxPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.f7 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.s2(self.c1(x)))\n",
    "        x = F.relu(self.s4(self.c3(x)))\n",
    "        \n",
    "        x = F.relu(self.c5(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.relu(self.f6(x))\n",
    "        x = self.f7(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, label\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement #5: Use Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 0.364424, Train Loss = 0.868267\n",
      "Epoch 1: Test Accuracy = 0.8932, Train Accuracy = 0.7449\n",
      "Epoch 2: Test Loss = 0.234410, Train Loss = 0.304749\n",
      "Epoch 2: Test Accuracy = 0.9304, Train Accuracy = 0.9092\n",
      "Epoch 3: Test Loss = 0.176403, Train Loss = 0.217226\n",
      "Epoch 3: Test Accuracy = 0.9460, Train Accuracy = 0.9347\n",
      "Epoch 4: Test Loss = 0.141677, Train Loss = 0.170702\n",
      "Epoch 4: Test Accuracy = 0.9583, Train Accuracy = 0.9480\n",
      "Epoch 5: Test Loss = 0.117975, Train Loss = 0.141245\n",
      "Epoch 5: Test Accuracy = 0.9625, Train Accuracy = 0.9575\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = nn.MaxPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.f7 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.s2(self.c1(x)))\n",
    "        x = F.relu(self.s4(self.c3(x)))\n",
    "        \n",
    "        x = F.relu(self.c5(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.relu(self.f6(x))\n",
    "        x = self.f7(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, label\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment, If we keep everything else the same and only change the normalization in the first code, Let's see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 1: Test Accuracy = 0.1026, Train Accuracy = 0.1044\n",
      "Epoch 2: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 2: Test Accuracy = 0.1026, Train Accuracy = 0.1044\n",
      "Epoch 3: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 3: Test Accuracy = 0.1026, Train Accuracy = 0.1044\n",
      "Epoch 4: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 4: Test Accuracy = 0.1026, Train Accuracy = 0.1044\n",
      "Epoch 5: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 5: Test Accuracy = 0.1026, Train Accuracy = 0.1044\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "class TrainableAvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        super(TrainableAvgPool2d, self).__init__()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size, stride=stride)\n",
    "        # Learnable weight and bias\n",
    "        self.weight = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "# Well guess what, RBFs are also extinct. So I gotta implement my own\n",
    "class RBFLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, gamma=1.0):\n",
    "        super(RBFLayer, self).__init__()\n",
    "        # Learnable RBF centers with shape [output_dim, input_dim]\n",
    "        self.centers = nn.Parameter(torch.randn(output_dim, input_dim))\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute squared Euclidean distance between input and RBF centers\n",
    "        # x shape: [batch_size, input_dim], centers shape: [output_dim, input_dim]\n",
    "        dists = torch.cdist(x.unsqueeze(1), self.centers.unsqueeze(0)) ** 2\n",
    "        # Apply Gaussian function to get RBF output\n",
    "        rbf_out = torch.exp(-self.gamma * dists.squeeze(1))\n",
    "        return rbf_out\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = TrainableAvgPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = TrainableAvgPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.rbf = RBFLayer(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.s2(self.c1(x)))\n",
    "        x = self.s4(self.c3(x))\n",
    "        x = self.c5(x)\n",
    "        x = x.view(-1, 120)\n",
    "        x = self.f6(x)\n",
    "        x = self.rbf(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, one_hot(torch.tensor(label), num_classes=10).float()\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0  # 0.0005 (initial LR)\n",
    "    elif epoch < 5:\n",
    "        return 0.4  # 0.0002\n",
    "    elif epoch < 8:\n",
    "        return 0.2  # 0.0001\n",
    "    elif epoch < 12:\n",
    "        return 0.1  # 0.00005\n",
    "    else:\n",
    "        return 0.02  # 0.00001\n",
    "\n",
    "    \n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.0005)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Convert y_true from one-hot to class indices\n",
    "    y_true_classes = y_true.argmax(dim=1)\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true_classes).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maybe try increasing learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 1: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 2: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 2: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 3: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 3: Test Accuracy = 0.1136, Train Accuracy = 0.1124\n",
      "Epoch 4: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 4: Test Accuracy = 0.1136, Train Accuracy = 0.1123\n",
      "Epoch 5: Test Loss = 0.100000, Train Loss = 0.100000\n",
      "Epoch 5: Test Accuracy = 0.1136, Train Accuracy = 0.1123\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "class TrainableAvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        super(TrainableAvgPool2d, self).__init__()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size, stride=stride)\n",
    "        # Learnable weight and bias\n",
    "        self.weight = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "# Well guess what, RBFs are also extinct. So I gotta implement my own\n",
    "class RBFLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, gamma=1.0):\n",
    "        super(RBFLayer, self).__init__()\n",
    "        # Learnable RBF centers with shape [output_dim, input_dim]\n",
    "        self.centers = nn.Parameter(torch.randn(output_dim, input_dim))\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute squared Euclidean distance between input and RBF centers\n",
    "        # x shape: [batch_size, input_dim], centers shape: [output_dim, input_dim]\n",
    "        dists = torch.cdist(x.unsqueeze(1), self.centers.unsqueeze(0)) ** 2\n",
    "        # Apply Gaussian function to get RBF output\n",
    "        rbf_out = torch.exp(-self.gamma * dists.squeeze(1))\n",
    "        return rbf_out\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = TrainableAvgPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = TrainableAvgPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.rbf = RBFLayer(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.s2(self.c1(x)))\n",
    "        x = self.s4(self.c3(x))\n",
    "        x = self.c5(x)\n",
    "        x = x.view(-1, 120)\n",
    "        x = self.f6(x)\n",
    "        x = self.rbf(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, one_hot(torch.tensor(label), num_classes=10).float()\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0  # 0.0005 (initial LR)\n",
    "    elif epoch < 5:\n",
    "        return 0.4  # 0.0002\n",
    "    elif epoch < 8:\n",
    "        return 0.2  # 0.0001\n",
    "    elif epoch < 12:\n",
    "        return 0.1  # 0.00005\n",
    "    else:\n",
    "        return 0.02  # 0.00001\n",
    "\n",
    "    \n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Convert y_true from one-hot to class indices\n",
    "    y_true_classes = y_true.argmax(dim=1)\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true_classes).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How about replace MSE Loss with Cross Entropy Loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 2.302585, Train Loss = 2.302585\n",
      "Epoch 1: Test Accuracy = 0.0894, Train Accuracy = 0.0904\n",
      "Epoch 2: Test Loss = 2.302585, Train Loss = 2.302585\n",
      "Epoch 2: Test Accuracy = 0.0894, Train Accuracy = 0.0903\n",
      "Epoch 3: Test Loss = 2.302585, Train Loss = 2.302585\n",
      "Epoch 3: Test Accuracy = 0.0894, Train Accuracy = 0.0904\n",
      "Epoch 4: Test Loss = 2.302585, Train Loss = 2.302585\n",
      "Epoch 4: Test Accuracy = 0.0894, Train Accuracy = 0.0904\n",
      "Epoch 5: Test Loss = 2.302585, Train Loss = 2.302585\n",
      "Epoch 5: Test Accuracy = 0.0894, Train Accuracy = 0.0903\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "class TrainableAvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        super(TrainableAvgPool2d, self).__init__()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size, stride=stride)\n",
    "        # Learnable weight and bias\n",
    "        self.weight = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "# Well guess what, RBFs are also extinct. So I gotta implement my own\n",
    "class RBFLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, gamma=1.0):\n",
    "        super(RBFLayer, self).__init__()\n",
    "        # Learnable RBF centers with shape [output_dim, input_dim]\n",
    "        self.centers = nn.Parameter(torch.randn(output_dim, input_dim))\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute squared Euclidean distance between input and RBF centers\n",
    "        # x shape: [batch_size, input_dim], centers shape: [output_dim, input_dim]\n",
    "        dists = torch.cdist(x.unsqueeze(1), self.centers.unsqueeze(0)) ** 2\n",
    "        # Apply Gaussian function to get RBF output\n",
    "        rbf_out = torch.exp(-self.gamma * dists.squeeze(1))\n",
    "        return rbf_out\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = TrainableAvgPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = TrainableAvgPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.rbf = RBFLayer(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.s2(self.c1(x)))\n",
    "        x = self.s4(self.c3(x))\n",
    "        x = self.c5(x)\n",
    "        x = x.view(-1, 120)\n",
    "        x = self.f6(x)\n",
    "        x = self.rbf(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, label\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0  # 0.0005 (initial LR)\n",
    "    elif epoch < 5:\n",
    "        return 0.4  # 0.0002\n",
    "    elif epoch < 8:\n",
    "        return 0.2  # 0.0001\n",
    "    elif epoch < 12:\n",
    "        return 0.1  # 0.00005\n",
    "    else:\n",
    "        return 0.02  # 0.00001\n",
    "\n",
    "    \n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remioving the RBF Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Loss = 0.351698, Train Loss = 1.139313\n",
      "Epoch 1: Test Accuracy = 0.8866, Train Accuracy = 0.6106\n",
      "Epoch 2: Test Loss = 0.306960, Train Loss = 0.329711\n",
      "Epoch 2: Test Accuracy = 0.9095, Train Accuracy = 0.9013\n",
      "Epoch 3: Test Loss = 0.220299, Train Loss = 0.253536\n",
      "Epoch 3: Test Accuracy = 0.9373, Train Accuracy = 0.9260\n",
      "Epoch 4: Test Loss = 0.169878, Train Loss = 0.205451\n",
      "Epoch 4: Test Accuracy = 0.9510, Train Accuracy = 0.9385\n",
      "Epoch 5: Test Loss = 0.134443, Train Loss = 0.164429\n",
      "Epoch 5: Test Accuracy = 0.9597, Train Accuracy = 0.9517\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "class TrainableAvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        super(TrainableAvgPool2d, self).__init__()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size, stride=stride)\n",
    "        # Learnable weight and bias\n",
    "        self.weight = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, 5, stride=1, padding=2,) # 1, 28, 28 -> 6, 28, 28\n",
    "        self.s2 = TrainableAvgPool2d(2, stride=2) # 6, 14, 14\n",
    "        self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.s4 = TrainableAvgPool2d(2, stride=2)\n",
    "        self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.fc7 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.s2(self.c1(x)))\n",
    "        x = self.s4(self.c3(x))\n",
    "        x = self.c5(x)\n",
    "        x = x.view(-1, 120)\n",
    "        x = self.f6(x)\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)   \n",
    "        return image, label\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0  # 0.0005 (initial LR)\n",
    "    elif epoch < 5:\n",
    "        return 0.4  # 0.0002\n",
    "    elif epoch < 8:\n",
    "        return 0.2  # 0.0001\n",
    "    elif epoch < 12:\n",
    "        return 0.1  # 0.00005\n",
    "    else:\n",
    "        return 0.02  # 0.00001\n",
    "\n",
    "    \n",
    "batch_size = 64\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', download=True, train=True)\n",
    "mnist_test = MNIST(root='./data', download=True, train=False)\n",
    "\n",
    "\n",
    "mnist_train_dataset = MNISTDataset(mnist_train, transforms)\n",
    "mnist_test_dataset = MNISTDataset(mnist_test, transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "model = Lenet5().cuda()\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred_classes = y_pred.argmax(dim=1)\n",
    "    return (y_pred_classes == y_true).float().mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_val = 0.0\n",
    "    train_acc_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_val += train_loss.item()\n",
    "        train_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    train_loss_val /= len(train_loader)\n",
    "    train_acc_val /= len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    test_acc_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            test_loss_val += loss(outputs, labels).item()\n",
    "            test_acc_val += accuracy(outputs, labels).item()\n",
    "\n",
    "    test_loss_val /= len(test_loader)\n",
    "    test_acc_val /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yup. So the problem is the RBF Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
